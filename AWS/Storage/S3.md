# S3 = Simple Storage Service

## Quick facts
- durable, safe place to store your files
- object-based storage
  * allows you to upload files
    - if successful, you'll receive a HTTP 200 code (if uploading via API or CLI)
  * each file up to 5 TB in size
  * unlimited storage
  * files are stored in "Buckets" (aka folders)
- _NOT_ for operating systems, DBs, etc.
- stored redundantly 
- __universal namespace__ -- names must be unique globally (like a URL)

## Details
- read after write consistency for _PUTS_ of new objects
  * aka you can access your files as soon as you upload them
- __eventual consistency__ for overwrite _PUTS_ & _DELETES_
  * aka these can take some time to propagate
- S3 is object-based
  * key = object name
  * value = data itself
  * version ID = important for versioning
  * metadata
  * subresources -- bucket-specific configuration:
    - bucket policies
    - access-control lists
    - cross-origin resource sharing (CORS)
    - transfer acceleration
- tiered storage available
- lifecycle management
- versioning
- encryption
- secure your data with __access control lists__ + __bucket policies__
- successful uploads = HTTP 200 status code

## Storage tiers / classes
- __regular S3__
  * 99.99% availability
    - aka service should be up 99.99% of the time
    - Amazon guarantees 99.9% availability
  * 99.999999999% (11x 9's) durability
  * stored redundantly across multiple devices & facilities
    - designed to sustain the loss of 2 facilities at the same time
- __S3 - IA (Infrequently Accessed)__
  * for data that's accessed less frequently, but requires rapid access when needed
  * lower fee than S3, but charged a retrieval fee
- __S3 - one-zone IA__
  * still 99.999999999% (11x 9's) durability
  * only 99.5% availability
  * cost is 20% off regular S3-IA
  * not resilient to loss of that Availability Zone
- __reduced-redundancy storage__
  * deisgned to provide 99.99% durability & 99.99% availability of objects over a given year
  * used for data that can be recreated if lost, e.g. thumbnail images
  * only offered in some regions; starting to get deprecated
- __Glacier__
  * very cheap, but used for archival only
  * optimized for data that's infrequently accessed
  * no real-time access; 4-5 hours to access
  * e.g. for historical documents
- __Intelligent Tiering__ (announced 11/2018)
  * 99.999999999% (11x 9's) durability, 99.9% availability
  * for unknown or unpredictable access patterns
  * 2 tiers
    - frequent access
    - infrequent access = has not been accessed in the last 30 days
  * automatically moves your data to the most cost-effective tier based on how frequently you access each object
  * no fees for accessing your data but a small monthly fee for monitoring / automation; $0.0025 per 1000 objects

## Charges
- storage per GB
- requests (get, put, copy, etc.)
- storage management pricing
  * inventory, analytics, and object tags
- data management pricing
  * data transferred out of S3, e.g. downloads
- transfer acceleration
  * uses CloudFront to optimize transfers

## Security
- by default, all newly-created buckets are _PRIVATE_
- you can set up access control to your buckets with:
  * __bucket policies__ ⟶ applied at a _bucket_ level
    - written in JSON
    - can use a policy-generator tool
  * __access-control lists__ ⟶ applied at an _object_ level
    - fine-grained control
    - can configure _access logs_ ⟶ these can be written to another S3 bucket 
- S3 buckets can be configured to create access logs, which log all requests made to the S3 bucket ⟶ these can be written to another S3 bucket

### types of encryption
- in-transit
  * __SSL / TLS (transport-layer security)__ ⟶ typically, this means HTTPS
- at-rest
  * server-side encryption
    - S3-managed keys = __SSE-S3__
    - AWS Key Management Service (KMS) -managed keys = __SSE-KMS__
      * uses an __*envelope key*__ to encrypt your data's encryption key; extra layer of security
      * has an audit trail
    - server-side encryption with customer-provided keys = __SSE-C__
      * AWS manages encryption / decryption for you but you manage your own keys
  * client-side encryption
    - you choose your own form of encryption and manage it yourself

### enforcing encryption on S3 buckets
- every time a file is uploaded to S3, a put request is initiated
```
PUT /myFile HTTP/1.1
Host: myBucket.s3.amazonaws.com
Date: date
Authorization: auth string
Content-Type: text/plain
Content-Length: 27272
x-amz-meta-author: yourUser
Expect: 100-continue
x-amz-server-side-encryption: AES256
[27272 bytes of object data]
```
- if the file is to be encrypted at upload time, the __`x-amz-server-side-encryption` parameter__ will be included in the request header
  * two options are currently available
    - `x-amz-server-side-encryption: AES256` ⟶ S3-managed keys
    - `x-amz-server-side-encryption: SSE-KMS` ⟶ KMS-managed keys
- you can enforce the use of server-side encryption by using a Bucket Policy that __*denies*__ any S3 _PUT_ request that does _NOT_ include the `x-amz-server-side-encryption` parameter in the request header

## S3 performance optimzation
- S3 is designed to support high request rates
- however, if you get 100+ PUT / LIST / DELETE requests/sec or 300+ GET requests/sec then you should optimize
- for GET-intensive workloads
  * use __transfer acceleration__ via CloudFront CDN for best performance
  * CloudFront will cache your most frequently-accessed objects & will reduce latency for GETs
- for mixed request type workloads (mix of GET, PUT, DELETE, GET bucket)
  * in the past
    - the key names you use can impact performance
    - S3 uses the key name to determine which partition an object will be stored in
    - using sequential keynames (e.g. timestamp prefixes or alphabetical sequence) increases likelihood of multiple objects being stored in the same partition ⟶ can cause I/O issues
    - a random prefix (e.g. hex hash) for keynames forces S3 to distribute keys across multiple partitions ⟶ distributes I/O workload
  * these days
    - S3 can handle 3300 PUT requests/sec & 5500 GET requests/sec
    - no need anymore to add random keynames

-----
# S3 Labs

## Lab: Access-Control Lists & Bucket Policies
- ACL = access-control list; add read / write access on a per AWS account basis
- lab: create S3 bucket
  * give it a DNS-compliant (aka unique URL) name
  * config options
    - can enable object-level logging (at additional $$)
    - can enable encryption (2 types)
      * SSE-S3 / AES-256 : server side encryption, with S3-managed keys
      * SSE-KMS / AWS-KM : server side encryption with KMS-managed keys
    - can enable CloudWatch request metrics
  * set permissions
    - public permissions
      * manage public ACL's for this bucket (2 options)
      * manage public bucket policies for this bucket (2 options)
      * __*NOTE: to enable public access, must uncheck all 4!*__
    - manage system permissions
      * (default) do not grant Amazon S3 Log Delivery group write access to this bucket
      * (or) enable server access logging
- lab: upload file
  * within file upload, you can add ACL on a per file (aka per object) basis
  * choose storage class
    - standard
    - standard-IA (infreq access; ~ 20% cheaper; charged per access)
    - one-zone IA
    - reduced redundancy (not recommended anymore)
  * can create metadata
  * can add tags
  * upload file ⟶ can then access by clicking "open" or via HTTPS link (if file made publicly readable)
- lab: add bucket policy
  * go to bucket ⟶ "public access settings" ⟶ uncheck all 4 protections related to public reads
  * __*NOTE: This does not necessarily mean files get public access! For a file to have public access, it must be marked "public readable" via both ACL & bucket policy.*__
- lab: bucket policy editor
  * you can write your own bucket policies in JSON with the AWS policy generator
    - select policy type = S3 bucket
    - effect = allow / deny
    - principal = IAM user or S3 bucket
    - AWS service = S3
    - actions = choose from list
    - ARN = resource name from bucket (copy-paste from bucket info page)
    - click "generate policy" ⟶ generates JSON, paste into S3 bucket policy editor

## Lab: Encryption
- lab: create bucket & enforce encryption on it (in this case, SSE-KMS)
  * (option) enable encryption via console upon creation
  * (or better) enforce encryption via bucket policy
    - build a policy with the policy generator
      * select policy type = S3
      * effect = deny
      * AWS service = S3
      * action selected = "put object"
      * add a special case:
        - condition = `StringNotEquals`
        - key = `s3x-amz-server-side-encryption`
        - value `aws:kms`
    - if you get the error message "action does not apply to resources in statement" ...
      * this is because S3 does not allow you to specify actions for individual resources (?) such as a folder
      * often resolved by adding a `"/*"` to the end of the folder path
- lab: try to ignore policy
  * upload file & do _NOT_ add encryption during upload ⟶ should fail with an "action not allowed" error
  * upload file & add KMS encryption ⟶ this should work
  * upload file & add S3 encryption ⟶ this should fail

## Lab: CORS configuration
- CORS = "cross-origin resource sharing"; when one resource in an S3 bucket wants to access a resource in another S3 bucket
- lab: create new bucket `testBucket`
  * configure as public bucket ⟶ create ⟶ go to "properties" tab ⟶ click "static website hosting"
  * add the `index.html` and `loadpage.html` files to that bucket
  * the `index.html` file should be publicly visible via the URL, and it should also pull in the info from `loadpage.html`
- lab: set up CORS
  * delete `loadpage.html` from `testBucket` and put it in a new bucket `testCorsBucket`
    - ⟶ repeat setup steps for the second bucket
    - ⟶ `index.html` from the first bucket should now return a 404 error since `loadpage.html` has changed location
  * to fix this (part 1) ...
    - ⟶ change `index.html` to access the public web endpoint of `loadpage.html`
    - ⟶ this should only load the HTML from `index.html`, and __*NOT*__ the other stuff, because we haven't yet set up CORS for `loadpage.html`
  * to fix this (part 2) ...
    - ⟶ copy the URL of `testBucket`
    - ⟶ go to `testCorsBucket`'s "permissions tab"
    - ⟶ click the "CORS configuration button"
    - ⟶ paste XML policy (from lab) with the URL of `testBucket` under "<AllowedOrigin>"
    - ⟶ now the HTML content in both files should load when you enter the URL for `index.html`	

-----

###### More resources:
- [S3 FAQs](https://aws.amazon.com/s3/faqs/)